# Enhancing Text Summarization of Biomedical Journal Papers with Domain-Specific Knowledge Integration in State-of-the-Art NLP Models

### Nilay Bhatt, Tom Shin, Luning Yang

## Abstract
The exponential growth of biomedical literature over the past decade has not just created a need,
but a pressing need for efficient summarization tools. These tools are crucial for researchers to stay
informed about recent developments in their field. As the volume and complexity of scientific papers
increase, automated summarization has become indispensable for researchers aiming to distill key
information rapidly. Although modern Natural Language Processing (NLP) models like BERT and
GPT have shown promising results in text summarization, they often need help to fully capture the
nuances and domain-specific language inherent in biomedical texts. This results in summaries that
lack accuracy or comprehensiveness, posing a significant challenge for researchers.
To address these challenges, this project not only leverages state-of-the-art NLP models, including
BART, T5, and LED, but also supplements them with domain-specific biomedical knowledge. This
unique approach is designed to enhance the summarization quality of biomedical journal papers.
By integrating specialized knowledge with these advanced models, we aim to not just improve the
accuracy and conciseness of summaries, but also make them contextually relevant. This will enable
researchers to navigate the rapidly expanding scientific literature more effectively. Our experimental
design involves in-domain and cross-domain summarization tasks to rigorously assess and refine our
models. Ultimately, our goal is to establish new benchmarks for summarization in this specialized
field, a significant step towards advancing biomedical literature summarization.
